Grupo 1
Se crea el archivo es_odio.py, que gestiona el manejo del comando advirtiendo en caso  de excepciones por falta de parámetros o archivos. Desde el mismo se llama una sola vez al entrenamiento, devolviendo un modelo, que será utilizado n veces para predecir cada uno de los archivos de testeo.
Se crea el archivo train.py que entrena el modelo. Carga los archivos de entrenamiento y validación, preprocesa los mismos (reemplazando nombres de usuarios por @usuario, las url por @url, eliminando hashtags y recortando las letras repetidas más de tres veces). Se crea un tokenizador ajustándose a las palabras de los tweets de entrenamiento. Se calcula la cantidad de palabras del tweet más largo del corpus de entrenamiento. 
Se preparan los tweets codificándolos como un vector con los tokens de sus palabras como los primeros elementos (tantas como la cantidad de palabras del tweet más largo antes calculado) y se agrega una nueva feature con la cantidad de plurales en el último lugar (creyendo que esto puede estar relacionado a los discursos de odio que tratan de discriminación o agresión a grupos de individuos). 
Se crea un modelo de red neuronal secuencial. Se define como primera capa de la red una capa Embedding usando como base los pesos provistos por el archivo fasttext.es.300.txt que recodifica la entrada a word embeddings. Siguiendo con una capa Flatten que condensa cada tweet a un solo valor y finalizando con una capa con la función de activación Sigmoide (durante la validación se usaron y descartaron varias capas: relu, elu y sigmoides ). Se compila el modelo usando el algoritmo adam para optimizar, entropía cruzada binaria como función de pérdida y reportando la precisión como métrica.
Se define un mecanismo de respaldo (Callback) que detiene el entrenamiento en caso de detectar overfitting y almacena el mejor resultado encontrado hasta el momento. Utilizando para esto el corpus de validación, la precisión obtenida en el mismo y definiendo que se corte el entrenamiento en caso de disminuir la precisión en 50 épocas seguidas (número determinado empíricamente).
Se entrena el modelo con 100 épocas.
Se predice para el corpus de test un acuracy de entre 60 y 63%
Se crea el archivo evaluate que dado un modelo y un archivo con tweets predice sus categorías, evalúa su desempeño y crea el archivo de salida con las categorías para cada tweet en una ruta especificada.
El manejo de vectores se realizó utilizando la librería numpy 1.18.2, el manejo de expresiones regulares para el preprocesamiento utilizó la librería estándar re, se tomaron las funcionalidades de redes neuronales de la librería keras 2.4.3.
